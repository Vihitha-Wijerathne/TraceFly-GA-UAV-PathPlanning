# -*- coding: utf-8 -*-
"""obstacle_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1640ahR8Rlbv1Mmk_erRXVgQ4TQhCDIg7

# LiDAR-Based Obstacle Detection Model (Optimized)

## Introduction

This document details the LiDAR-based obstacle detection model using deep learning architectures. The model integrates PointPillars and CenterPoint for improved object detection.

## Model Components

### Libraries and Dependencies

The following libraries are required for implementation:

*   `open3d` - Point cloud processing
*   `numpy` - Data manipulation
*   `tensorflow` - Deep learning framework
*   `matplotlib` - Visualization
*   `sklearn` - Model evaluation metrics
"""

# Install required libraries
!pip install open3d numpy matplotlib tensorflow

# Import necessary libraries
import os
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import open3d as o3d

"""## Data Handling

Loading Point Cloud Data from Google Drive
"""

# Google Drive Mount
from google.colab import drive
drive.mount('/content/drive')

# Paths to models and datasets
# Paths to validation data and models
validation_data_path = "/content/drive/MyDrive/validation_point_cloud.ply"
validation_labels_path = "/content/drive/MyDrive/validation_labels.txt"
pointpillars_model_path = "/content/drive/MyDrive/PointPillars_Object_Detection.h5"
centerpoint_model_path = "/content/drive/MyDrive/CenterPoint_Detection.h5"
pointrcnn_model_path = "/content/drive/MyDrive/PointRCNN_Detection.h5"

# Load validation point cloud
def load_point_cloud(file_path):
    """
    Load the point cloud from a .ply file using Open3D.
    """
    point_cloud = o3d.io.read_point_cloud(file_path)
    if len(point_cloud.points) == 0:
        raise ValueError("Point cloud file is empty or invalid.")
    points = np.asarray(point_cloud.points)
    return points

# Normalize points
def normalize_points(points):
    min_vals = np.min(points, axis=0)
    max_vals = np.max(points, axis=0)
    normalized_points = (points - min_vals) / (max_vals - min_vals)
    return normalized_points

# Load validation labels
def load_labels(file_path):
    with open(file_path, 'r') as f:
        labels = np.array([int(line.strip()) for line in f if not line.startswith("#")])
    return labels

# Preprocess points for general models
def preprocess_points(points, labels=None, num_features=3, pillar_size=3):
    """
    Preprocess points and optionally trim to match labels.
    """
    if labels is not None:
        points = points[:len(labels)]
    num_points = points.shape[0]
    reshaped_points = points[:num_points - (num_points % pillar_size)].reshape((-1, num_features, pillar_size))
    return reshaped_points

def preprocess_points_for_pointrcnn(points, labels):
    """
    Preprocess points to match the expected input shape of the PointRCNN model
    and ensure alignment with the number of labels.
    """
    # Trim points to match the number of labels
    points = points[:len(labels)]
    num_points = points.shape[0]

    # Add a dummy feature (e.g., intensity = 0) to make the input shape (None, 4)
    dummy_feature = np.zeros((num_points, 1))
    reshaped_points = np.hstack([points, dummy_feature])

    return reshaped_points

# Load validation point cloud and labels
validation_points = load_point_cloud(validation_data_path)
validation_points = normalize_points(validation_points)
validation_labels = load_labels(validation_labels_path)

print(f"Validation points shape: {validation_points.shape}")
print(f"Validation labels shape: {validation_labels.shape}")

# Preprocess for the models
processed_validation_points = preprocess_points(validation_points, validation_labels)

# One-hot encode validation labels
num_classes = max(validation_labels) + 1  # Determine the number of classes from labels
one_hot_validation_labels = tf.keras.utils.to_categorical(validation_labels[:processed_validation_points.shape[0]], num_classes=num_classes)
print(f"One-hot encoded validation labels shape: {one_hot_validation_labels.shape}")

# Load models
pointpillars_model = tf.keras.models.load_model(pointpillars_model_path)
centerpoint_model = tf.keras.models.load_model(centerpoint_model_path)
pointrcnn_model = tf.keras.models.load_model(pointrcnn_model_path)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np

def evaluate_model(model, points, labels):
    """
    Evaluate the model and calculate performance metrics.
    Convert one-hot encoded labels back to class indices if necessary.
    """
    # Predict class probabilities
    predictions = model.predict(points)

    # Convert predicted probabilities to class indices
    predicted_classes = np.argmax(predictions, axis=1)

    # Convert one-hot encoded true labels to class indices if needed
    if labels.ndim > 1:  # If labels are one-hot encoded
        labels = np.argmax(labels, axis=1)

    # Calculate metrics
    accuracy = accuracy_score(labels, predicted_classes)
    precision = precision_score(labels, predicted_classes, average='weighted')
    recall = recall_score(labels, predicted_classes, average='weighted')
    f1 = f1_score(labels, predicted_classes, average='weighted')

    return accuracy, precision, recall, f1, predicted_classes

results = {}

# Evaluate PointPillars
accuracy, precision, recall, f1, predicted_pp = evaluate_model(
    pointpillars_model, processed_validation_points, one_hot_validation_labels
)
results["PointPillars"] = [accuracy, precision, recall, f1]

# Evaluate CenterPoint
accuracy, precision, recall, f1, predicted_cp = evaluate_model(
    centerpoint_model, processed_validation_points, one_hot_validation_labels
)
results["CenterPoint"] = [accuracy, precision, recall, f1]

# Preprocess validation points for PointRCNN
processed_validation_points_rcnn = preprocess_points_for_pointrcnn(validation_points, validation_labels)

# Re-encode the labels to match the processed points
one_hot_validation_labels_rcnn = tf.keras.utils.to_categorical(
    validation_labels[:processed_validation_points_rcnn.shape[0]], num_classes=num_classes
)

# Evaluate PointRCNN
accuracy, precision, recall, f1, predicted_rcnn = evaluate_model(
    pointrcnn_model, processed_validation_points_rcnn, one_hot_validation_labels_rcnn
)
results["PointRCNN"] = [accuracy, precision, recall, f1]

# Define the plot_confusion_matrix function
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def plot_confusion_matrix(true_labels, predicted_labels, model_name):
    """
    Plot the confusion matrix for a given set of true and predicted labels.
    """
    cm = confusion_matrix(true_labels, predicted_labels)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))
    plt.title(f"Confusion Matrix for {model_name}")
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.show()

# Adjust labels to match predictions
def adjust_labels_for_predictions(labels, predictions):
    """
    Adjust the labels to match the size of predictions.
    """
    return labels[:len(predictions)]

# Adjust labels for each model
adjusted_labels_pp = adjust_labels_for_predictions(validation_labels, predicted_pp)
adjusted_labels_cp = adjust_labels_for_predictions(validation_labels, predicted_cp)
adjusted_labels_rcnn = adjust_labels_for_predictions(validation_labels, predicted_rcnn)

# Plot confusion matrix for each model
plot_confusion_matrix(adjusted_labels_pp, predicted_pp, "PointPillars")
plot_confusion_matrix(adjusted_labels_cp, predicted_cp, "CenterPoint")
plot_confusion_matrix(adjusted_labels_rcnn, predicted_rcnn, "PointRCNN")

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def evaluate_model(true_labels, predicted_labels, model_name):
    """
    Compute performance metrics for object detection models.
    """
    accuracy = accuracy_score(true_labels, predicted_labels)
    precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=1)
    recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=1)
    f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=1)

    print(f"{model_name} Performance:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}\n")

    return accuracy, precision, recall, f1

def plot_confusion_matrix(true_labels, predicted_labels, model_name):
    """
    Plot confusion matrix for visualization.
    """
    cm = confusion_matrix(true_labels, predicted_labels)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.title(f"Confusion Matrix for {model_name}")
    plt.show()

# --- Ensure Correct Label Formatting ---
def preprocess_labels(labels):
    """
    Ensure labels are integer arrays, not one-hot encoded.
    """
    labels = np.array(labels)  # Convert to NumPy array

    # Convert one-hot to categorical labels if needed
    if len(labels.shape) > 1 and labels.shape[1] > 1:
        labels = np.argmax(labels, axis=1)

    # Remove NaN values and ensure integer format
    labels = np.nan_to_num(labels, nan=0).astype(int)

    return labels

# --- Example Usage ---
num_samples = 100  # Replace with your actual number of samples
true_labels = np.random.randint(0, 3, num_samples)  # Replace with your actual data
predicted_pillars = np.random.randint(0, 3, num_samples)  # Replace with your actual predictions
predicted_center = np.random.randint(0, 3, num_samples)  # Replace with your actual predictions
predicted_rcnn = np.random.randint(0, 3, num_samples)  # Replace with your actual predictions


# Preprocess labels
true_labels = preprocess_labels(true_labels)
predicted_pillars = preprocess_labels(predicted_pillars)
predicted_center = preprocess_labels(predicted_center)
predicted_rcnn = preprocess_labels(predicted_rcnn)

# Print unique labels to verify
print("Unique true labels:", np.unique(true_labels))
print("Unique predicted labels (PointPillars):", np.unique(predicted_pillars))
print("Unique predicted labels (CenterPoint):", np.unique(predicted_center))
print("Unique predicted labels (PointRCNN):", np.unique(predicted_rcnn))

# --- Evaluate Models ---
results = {}
results['PointPillars'] = evaluate_model(true_labels, predicted_pillars, "PointPillars")
results['CenterPoint'] = evaluate_model(true_labels, predicted_center, "CenterPoint")
results['PointRCNN'] = evaluate_model(true_labels, predicted_rcnn, "PointRCNN")

# --- Plot Confusion Matrices ---
plot_confusion_matrix(true_labels, predicted_pillars, "PointPillars")
plot_confusion_matrix(true_labels, predicted_center, "CenterPoint")
plot_confusion_matrix(true_labels, predicted_rcnn, "PointRCNN")

# --- Visualize Performance Metrics ---
labels = list(results.keys())
accuracy_vals = [val[0] for val in results.values()]
precision_vals = [val[1] for val in results.values()]
recall_vals = [val[2] for val in results.values()]
f1_vals = [val[3] for val in results.values()]

plt.figure(figsize=(8, 5))
x = np.arange(len(labels))
width = 0.2
plt.bar(x - width, accuracy_vals, width=width, label='Accuracy')
plt.bar(x, precision_vals, width=width, label='Precision')
plt.bar(x + width, recall_vals, width=width, label='Recall')
plt.bar(x + 2*width, f1_vals, width=width, label='F1-score')
plt.xticks(ticks=x, labels=labels)
plt.ylabel("Scores")
plt.title("Model Performance Comparison")
plt.legend()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models

# Define a feature extraction module
class FeatureExtractor(nn.Module):
    def __init__(self, model_type="centerpoint"):
        super(FeatureExtractor, self).__init__()
        if model_type == "centerpoint":
            self.model = models.resnet18(pretrained=True)  # Placeholder for CenterPoint backbone
        elif model_type == "pointrcnn":
            self.model = models.resnet34(pretrained=True)  # Placeholder for PointRCNN backbone
        elif model_type == "pointpillars":
            self.model = models.resnet50(pretrained=True)  # Placeholder for PointPillars backbone
        else:
            raise ValueError("Unsupported model type")

        # Modify to select the appropriate output features
        self.feature_layer = nn.Sequential(*list(self.model.children())[:-1])
        # Reshape the output to have the desired dimensions (batch_size, input_dim)
    def forward(self, x):
        features = self.feature_layer(x)
        features = features.flatten(1)  # Flatten to (batch_size, num_features)
        return features

# Define Temporal Fusion using LSTMs (unchanged)
class TemporalFusion(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers=1):
        super(TemporalFusion, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, input_dim)  # Project back to input dimension

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])  # Use last output for prediction
        return out

# Instantiate models (unchanged)
centerpoint_extractor = FeatureExtractor("centerpoint")
pointrcnn_extractor = FeatureExtractor("pointrcnn")
pointpillars_extractor = FeatureExtractor("pointpillars")

# Update the input dimension for TemporalFusion
temporal_fusion = TemporalFusion(input_dim=3072, hidden_dim=256) # Updated input_dim to 1536 (3 * 512)

def process_batch(dummy_input):
    center_features = centerpoint_extractor(dummy_input)
    rcnn_features = pointrcnn_extractor(dummy_input)
    pillars_features = pointpillars_extractor(dummy_input)

    # Concatenate along the feature dimension (dim=1) to combine features
    combined_features = torch.cat((center_features, rcnn_features, pillars_features), dim=1)

    # Update the input dimension for TemporalFusion to match the combined features dimension
    temporal_fusion = TemporalFusion(input_dim=combined_features.shape[1], hidden_dim=256)

    # Add a time dimension for the LSTM (unsqueeze at dim=1)
    combined_features = combined_features.unsqueeze(1)

    output = temporal_fusion(combined_features)
    return output

# Placeholder input for testing
dummy_input = torch.randn(2, 3, 224, 224)  # Example batch (2 samples, 3 channels, 224x224 images)
out = process_batch(dummy_input)
print("Output shape:", out.shape)

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns


class FeatureExtractor(nn.Module):
    def __init__(self, model_type="centerpoint"):
        super(FeatureExtractor, self).__init__()
        if model_type == "centerpoint":
            self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)  # Placeholder
        elif model_type == "pointpillars":
            self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)  # Placeholder
        elif model_type == "pointrcnn":
            self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)  # Placeholder
        else:
            raise ValueError("Unsupported model type")

        self.feature_layer = nn.Sequential(*list(self.model.children())[:-1])  # Remove last FC layer

    def forward(self, x):
        features = self.feature_layer(x)
        features = features.flatten(1)  # Flatten to (batch_size, num_features)
        return features

# Load Feature Extractors
centerpoint_extractor = FeatureExtractor("centerpoint")
pointpillars_extractor = FeatureExtractor("pointpillars")
pointrcnn_extractor = FeatureExtractor("pointrcnn")

def process_batch(dummy_input, combination="dual"):
    # Extract Features
    center_features = centerpoint_extractor(dummy_input)
    pillars_features = pointpillars_extractor(dummy_input)
    rcnn_features = pointrcnn_extractor(dummy_input)

    # Different Combinations
    if combination == "pillars_center":  # PointPillars + CenterPoint
        combined_features = torch.cat((pillars_features, center_features), dim=1)
    elif combination == "pillars_rcnn":  # PointPillars + PointRCNN
        combined_features = torch.cat((pillars_features, rcnn_features), dim=1)
    elif combination == "center_rcnn":  # CenterPoint + PointRCNN
        combined_features = torch.cat((center_features, rcnn_features), dim=1)
    elif combination == "all_three":  # PointPillars + CenterPoint + PointRCNN
        combined_features = torch.cat((pillars_features, center_features, rcnn_features), dim=1)
    else:
        raise ValueError("Invalid Combination")

    return combined_features

# Generate synthetic labels
true_labels = np.random.randint(0, 3, size=(4,))  # Random classes [0,1,2]

# Simulated predictions for each model combination
predicted_pillars_center = np.random.randint(0, 3, size=(4,))
predicted_pillars_rcnn = np.random.randint(0, 3, size=(4,))
predicted_center_rcnn = np.random.randint(0, 3, size=(4,))
predicted_all_three = np.random.randint(0, 3, size=(4,))

# Compute Performance Metrics
def evaluate_model(true_labels, predicted_labels, model_name):
    accuracy = accuracy_score(true_labels, predicted_labels)
    precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=1)
    recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=1)
    f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=1)

    print(f"{model_name} Performance:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-score: {f1:.4f}\n")

    return accuracy, precision, recall, f1

# Evaluate models
results = {}
results['PointPillars + CenterPoint'] = evaluate_model(true_labels, predicted_pillars_center, "PointPillars + CenterPoint")
results['PointPillars + PointRCNN'] = evaluate_model(true_labels, predicted_pillars_rcnn, "PointPillars + PointRCNN")
results['CenterPoint + PointRCNN'] = evaluate_model(true_labels, predicted_center_rcnn, "CenterPoint + PointRCNN")
results['All Three Combined'] = evaluate_model(true_labels, predicted_all_three, "All Three Combined")

def plot_confusion_matrix(true_labels, predicted_labels, model_name):
    cm = confusion_matrix(true_labels, predicted_labels)
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.xlabel("Predicted Labels")
    plt.ylabel("True Labels")
    plt.title(f"Confusion Matrix for {model_name}")
    plt.show()

# Plot confusion matrices
plot_confusion_matrix(true_labels, predicted_pillars_center, "PointPillars + CenterPoint")
plot_confusion_matrix(true_labels, predicted_pillars_rcnn, "PointPillars + PointRCNN")
plot_confusion_matrix(true_labels, predicted_center_rcnn, "CenterPoint + PointRCNN")
plot_confusion_matrix(true_labels, predicted_all_three, "All Three Combined")

labels = results.keys()
accuracy_vals = [val[0] for val in results.values()]
precision_vals = [val[1] for val in results.values()]
recall_vals = [val[2] for val in results.values()]
f1_vals = [val[3] for val in results.values()]

plt.figure(figsize=(8, 5))
x = np.arange(len(labels))
width = 0.2
plt.bar(x - width, accuracy_vals, width=width, label='Accuracy')
plt.bar(x, precision_vals, width=width, label='Precision')
plt.bar(x + width, recall_vals, width=width, label='Recall')
plt.bar(x + 2*width, f1_vals, width=width, label='F1-score')
plt.xticks(ticks=x, labels=labels, rotation=10)
plt.ylabel("Scores")
plt.title("Model Performance Comparison")
plt.legend()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision import models
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# ----------------------------
# 1. FEATURE EXTRACTION MODULE
# ----------------------------
class FeatureExtractor(nn.Module):
    def __init__(self, model_type="centerpoint", output_dim=512):
        super(FeatureExtractor, self).__init__()

        if model_type == "centerpoint":
            self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
            feature_dim = 512
        elif model_type == "pointrcnn":
            self.model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)
            feature_dim = 512
        elif model_type == "pointpillars":
            self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
            feature_dim = 2048
        else:
            raise ValueError("Unsupported model type")

        # Remove classification layer
        self.feature_layer = nn.Sequential(*list(self.model.children())[:-1])

        # Projection layer to standardize feature dimensions to 512
        self.projection = nn.Linear(feature_dim, output_dim)

    def forward(self, x):
        features = self.feature_layer(x)
        features = features.flatten(1)  # Flatten features
        features = self.projection(features)  # Project to fixed 512 dimensions
        features = F.normalize(features, p=2, dim=1)  # Normalize features
        return features

# ----------------------------
# 2. FIXED SELF-ATTENTION FEATURE FUSION MODULE
# ----------------------------
class SelfAttentionFusion(nn.Module):
    def __init__(self, input_dim=512, output_dim=3):
        super(SelfAttentionFusion, self).__init__()
        self.attn_weights = nn.Linear(input_dim, input_dim)  # Fix: Match feature dim (512)
        self.fc1 = nn.Linear(input_dim, 256)
        self.fc2 = nn.Linear(256, output_dim)
        self.dropout = nn.Dropout(0.3)

    def forward(self, center_feat, rcnn_feat, pillars_feat):
        # Stack features for attention-based weighting
        stacked_features = torch.stack([center_feat, rcnn_feat, pillars_feat], dim=1)  # (batch, 3, 512)

        # Compute attention scores (batch, 3, 512)
        attn_scores = torch.softmax(self.attn_weights(stacked_features), dim=1)

        # Element-wise multiplication of attention scores & features
        weighted_features = torch.sum(attn_scores * stacked_features, dim=1)  # (batch, 512)

        # Pass through classifier
        x = self.dropout(F.relu(self.fc1(weighted_features)))
        x = self.fc2(x)
        return x

# ----------------------------
# 3. LOSS FUNCTION - FOCAL LOSS
# ----------------------------
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)  # Probabilities
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

# ----------------------------
# 4. MODEL INITIALIZATION
# ----------------------------
centerpoint_extractor = FeatureExtractor("centerpoint", output_dim=512)
pointrcnn_extractor = FeatureExtractor("pointrcnn", output_dim=512)
pointpillars_extractor = FeatureExtractor("pointpillars", output_dim=512)
fusion_module = SelfAttentionFusion(input_dim=512, output_dim=3)

# Optimizer & Loss
optimizer = optim.Adam(fusion_module.parameters(), lr=1e-4)
criterion = FocalLoss()

# ----------------------------
# 5. TRAINING LOOP
# ----------------------------
epochs = 10
batch_size = 16

for epoch in range(epochs):
    optimizer.zero_grad()

    # Dummy input data
    dummy_input = torch.randn(batch_size, 3, 224, 224)
    labels = torch.randint(0, 3, (batch_size,))

    # Extract features
    center_feat = centerpoint_extractor(dummy_input)
    rcnn_feat = pointrcnn_extractor(dummy_input)
    pillars_feat = pointpillars_extractor(dummy_input)

    # Fuse features and classify
    output = fusion_module(center_feat, rcnn_feat, pillars_feat)

    # Compute loss
    loss = criterion(output, labels)

    # Backpropagation
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# Save model
torch.save(fusion_module.state_dict(), "final_hybrid_model.pth")
print("Training Completed! Model Saved.")

# ----------------------------
# 6. EVALUATION & METRICS
# ----------------------------
fusion_module.load_state_dict(torch.load("final_hybrid_model.pth"))
fusion_module.eval()

# Generate validation data
batch_size = 16
dummy_input = torch.randn(batch_size, 3, 224, 224)
true_labels = torch.randint(0, 3, (batch_size,))

# Extract features
center_feat = centerpoint_extractor(dummy_input)
rcnn_feat = pointrcnn_extractor(dummy_input)
pillars_feat = pointpillars_extractor(dummy_input)

# Get predictions
with torch.no_grad():
    outputs = fusion_module(center_feat, rcnn_feat, pillars_feat)
    predicted_labels = torch.argmax(outputs, dim=1)

# Compute evaluation metrics
accuracy = accuracy_score(true_labels.numpy(), predicted_labels.numpy())
precision = precision_score(true_labels.numpy(), predicted_labels.numpy(), average='weighted', zero_division=1)
recall = recall_score(true_labels.numpy(), predicted_labels.numpy(), average='weighted', zero_division=1)
f1 = f1_score(true_labels.numpy(), predicted_labels.numpy(), average='weighted', zero_division=1)

print(f"\nHybrid Model Performance:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# ----------------------------
# 7. CONFUSION MATRIX
# ----------------------------
cm = confusion_matrix(true_labels.numpy(), predicted_labels.numpy())
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1, 2], yticklabels=[0, 1, 2])
plt.xlabel("Predicted Labels")
plt.ylabel("True Labels")
plt.title("Confusion Matrix for Hybrid Model")
plt.show()